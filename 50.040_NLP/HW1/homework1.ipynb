{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.040 Natural Language Processing (Fall 2024) Homework 1\n",
    "\n",
    "**Due 04 October 2024, 23:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### STUDNET ID:\n",
    "\n",
    "### Name:\n",
    "\n",
    "### Students with whom you have discussed (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Word embeddings are dense vectors that represent words, and capable of capturing semantic and syntactic similarity, relation with other words, etc.\n",
    "We have introduced two approaches in the class to learn word embeddings: **Count-based** and **Prediction-based**. \n",
    "Here we will explore both approaches. Note that we use \"word embeddings\" and \"word vectors\" interchangeably.\n",
    "\n",
    "-------\n",
    "\n",
    "Before we start, you need to [download](http://mattmahoney.net/dc/text8.zip) the text8 dataset. Unzip the file and then put it under the \"data\" folder. The text8 dataset consists of one single line of long text. Please do not change the data unless you are requested to do so.\n",
    "\n",
    "Environment:\n",
    "- Python 3.5 or above\n",
    "- gensim \n",
    "- sklearn\n",
    "- numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count-Based Word Embeddings\n",
    "\n",
    "### Co-Occurrence \n",
    "\n",
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the *context window* surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \\dots w_{i-1}$ and $w_{i+1} \\dots w_{i+n}$. We build a *co-occurrence matrix* $M$, which is a symmetric word-by-word matrix in which $m_{ij}$ is the number of times $w_j$ appears inside $w_i$'s window.\n",
    "\n",
    "**Example: Co-Occurrence with Fixed Window of n=1**:\n",
    "\n",
    "Document 1: \"learn and live\"\n",
    "\n",
    "Document 2: \"learn not and know not\"\n",
    "\n",
    "|     *    | and | know | learn | live | not |\n",
    "|----------|-----|------|-------|------|-----|\n",
    "| and      | 0   | 1    | 1     | 1    | 1   |\n",
    "| know     | 1   | 0    | 0     | 0    | 1   |\n",
    "| learn    | 1   | 0    | 0     | 0    | 1   |\n",
    "| live     | 1   | 0    | 0     | 0    | 0   |\n",
    "| not      | 1   | 1    | 1     | 0    | 0   |\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 [written]:\n",
    "To have a better understanding of the co-occurrence matrix, please write a matrix $M$ with fixed window of $n = 2$:\n",
    "\n",
    "Document 1: \"learn to know\"\n",
    "\n",
    "Document 2: \"not learn to not know well\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Write your answer:***\n",
    "\n",
    "|     *    | know | learn | not | to | well |\n",
    "|----------|-----|------|-------|------|-----|\n",
    "| know     |    |     |      |     |    |\n",
    "| learn    |    |     |      |     |    |\n",
    "| not      |    |     |      |     |    |\n",
    "| to       |   |     |      |     |    |\n",
    "| well      |    |     |     |     |    |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Pointwise Mutual Information (PPMI)\n",
    "Pointwise mutual information (PMI) is one of the most important concepts in NLP. The pointwise mutual information between a target word $w$ and a context word $c$ is defined as:\n",
    "\\begin{align}\n",
    "\\operatorname{PMI}(w, c)=\\log _{2} \\frac{P(w, c)}{P(w) P(c)}\\notag\n",
    "\\end{align}\n",
    "It is more common to use positive PMI (PPMI) which replaces all negative PMI values with zero. \n",
    "\n",
    "Given co-occurrence matrix $\\mathbf M\\in{\\mathbb Z}^{N\\times N}$ of $N$ words, $m_{ij}$ is the element of $i$ th row and $j$ th column. The PPMI matrix can be calculated as \n",
    "\\begin{align}\n",
    "\\operatorname{PPMI}_{i j}=\\max \\left(\\log _{2} \\frac{p_{i j}}{p_{i *} p_{* j}}, 0\\right)\\notag\n",
    "\\end{align}\n",
    "where \n",
    "\\begin{align}\n",
    "p_{i j}=\\frac{m_{i j}}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} m_{i j}}\\quad  p_{i *}=\\frac{\\sum_{j=1}^{N} m_{i j}}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} m_{i j}} \\quad p_{* j}=\\frac{\\sum_{i=1}^{N} m_{i j}}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} m_{i j}}\\notag\n",
    "\\end{align}\n",
    "For the details of PMI and PPMI, please refer to https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "### Principal Components Analysis (PCA) and Truncated Singular Value Decomposition (Truncated SVD) \n",
    "The rows (or columns) of co-occurrence matrix or PPMI matrix can be utilized as word vectors, but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run dimensionality reduction. In particular, we will first run PCA (Principal Components Analysis) to reduce the dimension. In practice, it is challenging to apply PCA to large corpora because of the memory needed to perform PCA. However, if you only want the top $k$ vector components for relatively small $k$ — known as Truncated SVD — then there are reasonably scalable techniques to compute those iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Corpus\n",
    "\n",
    "Before you start, please make sure you have downloaded the dataset \"text8\" in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(file_path, size=50000):\n",
    "    '''\n",
    "    params:\n",
    "        file_path --- str: path to your data file.\n",
    "        size --- int or str: the size of the corpus\n",
    "    return:\n",
    "        corpus --- list[str]: list of word strings.\n",
    "    '''\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "        if size=='all':\n",
    "            corpus = text.split()\n",
    "        else:\n",
    "            corpus = text.split()[:size]\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(r'./data/text8')\n",
    "print(corpus[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 [code]:\n",
    "Implement the function \"distinct_words\" that reads in \"corpus\" and returns distinct words that appeared in the corpus and the number of distinct words. \n",
    "\n",
    "Then, run the sanity check cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" \n",
    "    Determine a list of distinct words for the corpus.\n",
    "    Params:\n",
    "        corpus --- list[str]: list of words in the corpus\n",
    "    Return:\n",
    "        corpus_words --- list[str]: list of distinct words in the corpus; sort this list with built-in python function \"sorted\"\n",
    "        num_corpus_words --- int: number of distinct words in the corpus\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    corpus_words = sorted(list(set(corpus)))\n",
    "    num_corpus_words = len(corpus_words)\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return corpus_words, num_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Passed All Tests!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Run this sanity check to check your implementation\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Define toy corpus\n",
    "test_corpus = \"learn and live\".split() + \"learn not and know not\".split()\n",
    "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
    "\n",
    "# Correct answers\n",
    "ans_test_corpus_words = sorted(list(set(['learn','and','live','not','know'])))\n",
    "ans_num_corpus_words = len(ans_test_corpus_words)\n",
    "\n",
    "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
    "\n",
    "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3 [code]: \n",
    "Implement \"compute_word_matrix\" that reads in \"corpus\" and \"window_size\", and returns a co-occurrence matrix, PPMI matrix and a word-to-index dictionary.\n",
    "\n",
    "Then, run the sanity check cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_word_matrix(corpus, window_size=1):\n",
    "    \"\"\" \n",
    "    Compute co-occurrence matrix and PPMI matrix for the given corpus and window_size (default of 1).\n",
    "    \n",
    "    Params:\n",
    "        corpus --- list[str]: list of words\n",
    "        window_size --- int: size of context window\n",
    "    Return:\n",
    "        CoM --- numpy array of shape (num_words, num_words): \n",
    "                Co-occurrence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words \n",
    "                given by the distinct_words function.\n",
    "        PPMI--- numpy array of shape (num_words, num_words): \n",
    "                PPMI matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words \n",
    "                given by the distinct_words function.\n",
    "                \n",
    "              \n",
    "        word2Ind --- dict: dictionary that maps word to index (i.e. row/column number) for matrix CoM which is the same as PPMI.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    CoM = None\n",
    "    PPMI = None\n",
    "    word2Ind = {}\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    for i in range(num_words):\n",
    "        word2Ind[words[i]] = i\n",
    "    CoM = np.zeros((num_words,num_words))\n",
    "    PPMI = np.zeros((num_words,num_words))\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        if corpus[i] in word2Ind:\n",
    "            for j in range(max(i-window_size,0),min(i+window_size+1,len(corpus))):\n",
    "                if i!=j and corpus[j] in word2Ind:\n",
    "                    CoM[word2Ind[corpus[i]]][word2Ind[corpus[j]]] += 1\n",
    "    total = np.sum(CoM)\n",
    "\n",
    "    # Calculate PMI using CoM\n",
    "    pij = CoM / total\n",
    "    pi = np.sum(CoM,axis=1) / total\n",
    "    pj = np.sum(CoM,axis=0) / total\n",
    "\n",
    "    # Ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "    for i in range(num_words):\n",
    "        for j in range(num_words):\n",
    "            if pij[i][j] / (pi[i] * pj[j]) > 0:\n",
    "                PPMI[i][j] = max(0,math.log(pij[i][j] / (pi[i] * pj[j]), 2))\n",
    "\n",
    "\n",
    "    ### END OF YOUR CODE   \n",
    "    return CoM,PPMI,word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct PPMI:\n",
      "[[0.        1.        0.        0.4150375]\n",
      " [1.        0.        0.        0.4150375]\n",
      " [0.        0.        0.        1.4150375]\n",
      " [0.4150375 0.4150375 1.4150375 0.       ]]\n",
      "Your PPMI: \n",
      "[[0.         0.69314718 0.         0.28768207]\n",
      " [0.69314718 0.         0.         0.28768207]\n",
      " [0.         0.         0.         0.98082925]\n",
      " [0.28768207 0.28768207 0.98082925 0.        ]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Incorrect count at index (0, 1)=(and, know) in matrix PPMI. Yours has 0.6931471805599453 but should have 1.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour PPMI: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;28mprint\u001b[39m(PPMI_test)\n\u001b[1;32m---> 51\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect count at index (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)=(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) in matrix PPMI. Yours has \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m but should have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx1, idx2, w1, w2, student2, correct2))\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Print Success\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Incorrect count at index (0, 1)=(and, know) in matrix PPMI. Yours has 0.6931471805599453 but should have 1.0."
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus and get co-occurrence matrix\n",
    "test_corpus = \"learn not and know not\".split()\n",
    "CoM_test, PPMI_test, word2Ind_test = compute_word_matrix(test_corpus, window_size=1)\n",
    "# Correct M and word2Ind\n",
    "CoM_test_ans = np.array( \n",
    "    [[0., 1., 0., 1.],\n",
    "     [1., 0., 0., 1.],\n",
    "     [0., 0., 0., 1.],\n",
    "     [1., 1., 1., 0.]])\n",
    "\n",
    "PPMI_test_ans = np.array(\n",
    "    [[0., 1., 0., math.log(4/3,2)],\n",
    "     [1., 0., 0., math.log(4/3,2)],   \n",
    "     [0., 0., 0., math.log(8/3,2)],   \n",
    "     [math.log(4/3,2), math.log(4/3,2), math.log(8/3,2), 0.]])\n",
    "\n",
    "word2Ind_ans = {'and':0, 'know':1, 'learn':2,  'not':3}\n",
    "\n",
    "# check correct word2Ind\n",
    "assert (word2Ind_ans == word2Ind_test), \"Your word2Ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2Ind_ans, word2Ind_test)\n",
    "\n",
    "# check correct CoM shape\n",
    "assert (CoM_test.shape == CoM_test_ans.shape), \"CoM matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(CoM_test.shape, CoM_test_ans.shape)\n",
    "\n",
    "# check correct PPMI shape\n",
    "assert (PPMI_test.shape == PPMI_test_ans.shape), \"PPMI matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(PPMI_test.shape, PPMI_test_ans.shape)\n",
    "# Test correct CoM and PPMI values\n",
    "for w1 in word2Ind_ans.keys():\n",
    "    idx1 = word2Ind_ans[w1]\n",
    "    for w2 in word2Ind_ans.keys():\n",
    "        idx2 = word2Ind_ans[w2]\n",
    "        student1 = CoM_test[idx1, idx2]\n",
    "        correct1 = CoM_test_ans[idx1, idx2]\n",
    "        student2 = PPMI_test[idx1, idx2]\n",
    "        correct2 = PPMI_test_ans[idx1, idx2]\n",
    "        if student1 != correct1:\n",
    "            print(\"Correct CoM:\")\n",
    "            print(CoM_test_ans)\n",
    "            print(\"Your CoM: \")\n",
    "            print(CoM_test)\n",
    "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix CoM. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student1, correct1))\n",
    "        if student2 != correct2:\n",
    "            print(\"Correct PPMI:\")\n",
    "            print(PPMI_test_ans)\n",
    "            print(\"Your PPMI: \")\n",
    "            print(PPMI_test)\n",
    "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix PPMI. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student2, correct2))\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4 [code]:\n",
    "Implement \"dimension_reduction\" function below with python package sklearn.decomposition. For the use of PCA function and TruncatedSVD function, please refer to https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
    "\n",
    "Then, run the sanity check cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dimension_reduction (X, k=2):\n",
    "    '''\n",
    "    params:\n",
    "        X --- numpy array of shape (num_words, word_embedding_size)\n",
    "        k --- int: the number of principal components that we keep\n",
    "    return:\n",
    "        X_reduced --- numpy array of shape (num_words, k)\n",
    "                      Using TruncatedSVD algorithm when k <= floor(word_embedding_size/10)\n",
    "                      Using PCA algorithm when k > floor(word_embedding_size/10)\n",
    "    '''\n",
    "    X_reduced = None\n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    ### YOUR CODE HERE \n",
    "\n",
    "    if k <= X.shape[1] // 10:\n",
    "        svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
    "        X_reduced = svd.fit_transform(X)\n",
    "    else:\n",
    "        pca = PCA(n_components=k)\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# only check that your M_reduced has the right dimensions.\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus and run student code\n",
    "test_corpus = \"learn not and know not\".split()\n",
    "CoM_test, PPMI_test, word2Ind_test = compute_word_matrix(test_corpus, window_size=1)\n",
    "CoM_test_reduced = dimension_reduction(CoM_test, k=2)\n",
    "\n",
    "# Test proper dimensions\n",
    "assert (CoM_test_reduced.shape[0] == 4), \"CoM_reduced has {} rows; should have {}\".format(CoM_test_reduced.shape[0], 4)\n",
    "assert (CoM_test_reduced.shape[1] == 2), \"CoM_reduced has {} columns; should have {}\".format(CoM_test_reduced.shape[1], 2)\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5 [code]:\n",
    "Implement \"plot_embeddings\" function to visualize the word embeddings on a 2-D plane. \n",
    "\n",
    "Then, run the sanity check cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(X_reduced, word2Ind, words, fig_size, fig_title):\n",
    "    \"\"\" \n",
    "    Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
    "        \n",
    "    params:\n",
    "        X_reduced --- numpy array of shape (num_words , 2): numpy array of 2-d word embeddings\n",
    "        word2Ind --- dict: dictionary that maps words to indices\n",
    "        words --- list[str]: a list of words of which the embeddings we want to visualize\n",
    "        fig_size --- tuple (a,b) : the size of figure\n",
    "        fig_title --- str: title of the figure\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = fig_size)\n",
    "    ### YOUR CODE HERE \n",
    "    for word in words:\n",
    "        idx = word2Ind[word]\n",
    "        plt.scatter(X_reduced[idx, 0], X_reduced[idx, 1], color='b')\n",
    "        plt.text(X_reduced[idx, 0], X_reduced[idx, 1], word, fontsize=9)\n",
    "    plt.title(fig_title)\n",
    "    plt.show()\n",
    "\n",
    "    ### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# Note that this not an exhaustive check for correctness.\n",
    "# The plot produced should look like the \"test solution plot\" depicted below. \n",
    "# ---------------------\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print (\"Outputted Plot:\")\n",
    "\n",
    "X_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
    "word2Ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
    "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
    "plot_embeddings(X_test, word2Ind_plot_test, words, (6,4), 'test_figure')\n",
    "\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=red>**Test Plot Solution**</font>\n",
    "<br>\n",
    "<img src=\"test_plot.png\" width=40% style=\"float: left;\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Run This Cell to Produce Your Plot\n",
    "# window_size is 3\n",
    "# ------------------------------\n",
    "\n",
    "corpus = read_corpus(r'./data/text8',50000)\n",
    "\n",
    "CoM, PPMI, word2Ind = compute_word_matrix(corpus, window_size=3)\n",
    "CoM_reduced = dimension_reduction(CoM, k=2)\n",
    "PPMI_reduced = dimension_reduction(PPMI, k=2)\n",
    "\n",
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "CoM_lengths = np.linalg.norm(CoM_reduced, axis=1)\n",
    "CoM_normalized = CoM_reduced / CoM_lengths[:, np.newaxis] # broadcasting\n",
    "PPMI_lengths = np.linalg.norm(PPMI_reduced, axis=1)\n",
    "PPMI_normalized = PPMI_reduced / PPMI_lengths[:, np.newaxis] # broadcasting\n",
    "\n",
    "\n",
    "words = ['king','man','woman','women','queen','prince']\n",
    "plot_embeddings(CoM_normalized, word2Ind, words, (8,5), 'Co-occurance embeddings of dimention 2')\n",
    "plot_embeddings(PPMI_normalized, word2Ind, words, (8,5), 'PPMI embeddings of dimention 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction-Based Word Embeddings\n",
    "### Word2vec\n",
    "Word2vec is a software package that contains two algorithms named CBOW and skip-gram ([Mikolov 2013](https://arxiv.org/pdf/1301.3781.pdf)). In the CBOW architecture, the model predicts the current word from a window of surrounding context words. In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The architectures are shown as follows:\n",
    "<br>\n",
    "<img src=\"Word2vec.png\" width=60% style=\"float: left;\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 [code]:\n",
    "Complete the code in the function *create_word_batch*, which can be used to divide a single sequence of words into batches of words. \n",
    "\n",
    "For example, the word sequence [\"I\", \"like\", \"NLP\", \"So\", \"does\", \"he\"] can be divided into two batches, [\"I\", \"like\", \"NLP\"], [\"So\", \"does\", \"he\"], each with batch_size=3 words. It is more efficient to train word embedding on batches of word sequences rather than on a long single sequence. \n",
    "\n",
    "Then, run the sanity check cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_word_batch(words, batch_size=100):\n",
    "    '''\n",
    "    Split the words into batches\n",
    "    params:\n",
    "        words --- list[str]: a list of words\n",
    "        batch_size --- int: the number of words in a batch\n",
    "    return:\n",
    "        batch_words: list[list[str]]batches of words, list\n",
    "    '''\n",
    "    batch_words = []\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    for i in range(0, len(words), batch_size):\n",
    "        batch_words.append(words[i:i + batch_size])\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return batch_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Run this sanity check to check your implementation\n",
    "# --------------------------------------------------\n",
    "words_test = [\"I\", \"like\", \"NLP\", \"So\", \"does\", \"he\"]\n",
    "batch_size_test = 3\n",
    "\n",
    "ans = [[\"I\", \"like\", \"NLP\"],[\"So\", \"does\", \"he\"]]\n",
    "\n",
    "batch_words_test = create_word_batch(words_test,batch_size_test)\n",
    "\n",
    "assert ans == batch_words_test, 'your output does not match \"ans\"'\n",
    "print('passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 [code]:\n",
    "Use \"Word2Vec\" function to build a word2vec model. For the use of \"Word2Vec\" function, please ,refer to https://radimrehurek.com/gensim/models/word2vec.html. Please use the parameters we have set for you.\n",
    "\n",
    "It may take a few minutes to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whole_corpus = corpus = read_corpus(r'./data/text8', 'all')\n",
    "batch_words = create_word_batch(whole_corpus)\n",
    "\n",
    "vector_size = 100\n",
    "min_count = 2\n",
    "window = 3\n",
    "sg = 1  #skip-gram algorithm\n",
    "### YOUR CODE HERE \n",
    "model = Word2Vec(batch_words, vector_size=vector_size, min_count=min_count, window=window, sg=sg)\n",
    "\n",
    "### END OF YOUR CODE\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3 [code]:\n",
    "Implement \"get_word2Ind\" function below first. Then, run the sanity check cell to check your implementation.\n",
    "\n",
    "Use \"get_word2Ind\", \"dimension_reduction\", and \"plot_embeddings\" functions to visualize the word embeddings of the first 300 words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_word2Ind(index2word):\n",
    "    '''\n",
    "    construct a dictionary that maps words to its index\n",
    "    \n",
    "    params:\n",
    "        index2word --- list[str]: list of words\n",
    "    return \n",
    "        word2index --- dict: keys are words, values are the corresponding indices\n",
    "    '''\n",
    "    word2index = dict()\n",
    "    ### YOUR CODE HERE\n",
    "    word2index = {word: i for i, word in enumerate(index2word)}\n",
    "    ### END OF YOUR CODE\n",
    "    return word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Run this sanity check to check your implementation\n",
    "# --------------------------------------------------\n",
    "i2w_test = ['I','love','it']\n",
    "ans_test = get_word2Ind(i2w_test)\n",
    "\n",
    "ans = {'I':0, 'love':1, 'it':2}\n",
    "assert ans == ans_test, 'your output did not match the correct answer.'\n",
    "print('passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index2word = model.wv.index_to_key\n",
    "words_to_visualize = index2word[:300]\n",
    "\n",
    "### YOUR CODE HERE\n",
    "word2Ind = get_word2Ind(index2word)\n",
    "word_vectors = model.wv.vectors\n",
    "words_to_visualize = words_to_visualize\n",
    "word_vectors_to_visualize = np.array([word_vectors[word2Ind[word]] for word in words_to_visualize])\n",
    "word_vectors_to_visualize_reduced = dimension_reduction(word_vectors_to_visualize, k=2)\n",
    "plot_embeddings(word_vectors_to_visualize_reduced, word2Ind, words_to_visualize, (10,10), 'Word2Vec embeddings of dimention 2')\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4 [code]:\n",
    "1. Find the most similar words for the given words \"dog\",\"car\",\"man\". You need to use \"model.wv.most_similar\" function.\n",
    "2. Find out which word will it be for x in the pairs woman : king :: man : x? You need to use \"model.wv.most_similar\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words =  ['dog', 'car', 'man'] \n",
    "### 1\n",
    "### YOUR CODE HERE \n",
    "model.wv.most_similar(words)\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2\n",
    "### YOUR CODE HERE \n",
    "words = ['woman', 'king', 'man']\n",
    "model.wv.most_similar(positive=[words[0], words[1]], negative=[words[2]])\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5 [code+written]:\n",
    "\n",
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
    "\n",
    "Use the `most_similar` function to find two cases where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE \n",
    "'''\n",
    "### Question 2.5 [code+written]:\n",
    "\n",
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
    "\n",
    "Use the `most_similar` function to find two cases where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover.\n",
    "'''\n",
    "# Example 1\n",
    "doctor_similar = model.most_similar('doctor')\n",
    "nurse_similar = model.most_similar('nurse')\n",
    "\n",
    "print(\"Words most similar to 'doctor':\")\n",
    "for word, similarity in doctor_similar:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "print(\"\\nWords most similar to 'nurse':\")\n",
    "for word, similarity in nurse_similar:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "# Example 2: Racial bias\n",
    "# Find words most similar to 'programmer' and 'janitor'\n",
    "programmer_similar = model.most_similar('programmer')\n",
    "janitor_similar = model.most_similar('janitor')\n",
    "\n",
    "print(\"\\nWords most similar to 'programmer':\")\n",
    "for word, similarity in programmer_similar:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "print(\"\\nWords most similar to 'janitor':\")\n",
    "for word, similarity in janitor_similar:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Write your explanation:***\n",
    "1. Gender Bias:\n",
    "   - When examining the words most similar to 'doctor' and 'nurse', we often find that 'doctor' is associated with male-dominated terms, while 'nurse' is associated with female-dominated terms. This reflects a gender bias in the word embeddings, reinforcing the stereotype that doctors are male and nurses are female.\n",
    "\n",
    "2. Racial Bias:\n",
    "   - Similarly, when looking at the words most similar to 'programmer' and 'janitor', 'programmer' might be associated with terms that reflect higher socioeconomic status and certain racial groups, while 'janitor' might be associated with terms reflecting lower socioeconomic status and different racial groups. This demonstrates a racial bias in the word embeddings, reinforcing stereotypes about certain professions being linked to specific racial groups.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
