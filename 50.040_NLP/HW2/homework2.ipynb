{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.040 Natural Language Processing (Summer 2024) Homework 2\n",
    "\n",
    "**Due October,28,2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### STUDENT ID: 1006184\n",
    "\n",
    "### Name: Atul Parida\n",
    "\n",
    "### Students with whom you have discussed (if any): Anutham Mukunthan, Ansh Oswal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Constituency parsing aims to extract a constituency-based parse tree from a sentence that represents \n",
    "its syntactic structure according to a phrase structure grammar.\n",
    "A typical constituency parse tree is shown below:\n",
    "\n",
    "![tree](parse_tree.png)\n",
    "\n",
    "$S$ is a distinguished start symbol, node labels such as $NP$(noun phrase), $VP$(verb phrase) are non-terminal symbols, leaf labels such as \"a\", \"banana\" are terminal symbols.\n",
    "\n",
    "In this homework, we will implement a constituency parser based on probabilistic context-free grammars (PCFGs) and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will be using a version of the \n",
    "\"Penn Treebank\"\n",
    "released in NLTK corpora to induce PCFGs and evaluate our algorithm. \n",
    "The preprocessing code has been provided, do not make any changes to the text\n",
    "and code unless you are requested to do so. Run the code we provide to load the training and\n",
    "test sets as Python lists, it will take 1 minute.\n",
    "Since we will not tune hyper-parameters in this homework, there will be no need for a development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCFGs\n",
    "A probabilistic context-free grammar consists of:\n",
    "\n",
    "- A context-free grammar $G=(N, \\ \\Sigma, \\ S, \\ R)$ where $N$ is a finite set of non-terminal symbols, $\\Sigma$ is a finite set of terminal symbols, $R$ is a finite set of rules (e.g., $NP \\rightarrow NP \\ PP$), $S \\in N$ is the start symbol.\n",
    "- One parameter $q(A \\rightarrow \\beta)$ for each rule $A \\rightarrow \\beta$ in $R$. Since the grammar is in Chomsky normal form, there are only two types of rules: $A \\rightarrow B \\ C$ and $A \\rightarrow \\alpha$, where $A$, $B$, $C \\in N$, $\\alpha \\in \\Sigma$.\n",
    "\n",
    "We can estimate the parameter $q(A \\rightarrow \\beta)$ using maximum likelihood estimation:\n",
    "\\begin{equation}\n",
    "\tq_{MLE}(A \\rightarrow \\beta) = \\frac {count(A \\rightarrow \\beta)}{count(A)}\n",
    "\\end{equation}\n",
    "where $count(A \\rightarrow \\beta)$ refers to the number of occurences of \n",
    "the rule $A \\rightarrow \\beta$ in all \n",
    "the parse trees in the training set, and  $count(A)$  refers to the number of occurences of\n",
    "the non-terminal symbol $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree\n",
    "from nltk import Nonterminal\n",
    "from nltk.corpus import LazyCorpusLoader, BracketParseCorpusReader\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\atulp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_leave_lower(tree_string):\n",
    "    if isinstance(tree_string, Tree):\n",
    "        tree = tree_string\n",
    "    else:\n",
    "        tree = Tree.fromstring(tree_string)\n",
    "    for idx, _ in enumerate(tree.leaves()):\n",
    "        tree_location = tree.leaf_treeposition(idx)\n",
    "        non_terminal = tree[tree_location[:-1]]\n",
    "        non_terminal[0] = non_terminal[0].lower()\n",
    "    return tree\n",
    "\n",
    "def get_train_test_data():\n",
    "    '''\n",
    "    Load training and test set from nltk corpora\n",
    "    '''\n",
    "    train_num = 3900\n",
    "    test_index = range(10)\n",
    "    treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
    "    cnf_train = treebank.parsed_sents()[:train_num]\n",
    "    cnf_test = [treebank.parsed_sents()[i+train_num] for i in test_index]\n",
    "    #Convert to Chomsky norm form, remove auxiliary labels\n",
    "    cnf_train = [convert2cnf(t) for t in cnf_train]\n",
    "    cnf_test = [convert2cnf(t) for t in cnf_test]\n",
    "    return cnf_train, cnf_test\n",
    "def convert2cnf(original_tree):\n",
    "    '''\n",
    "    Chomsky norm form\n",
    "    '''\n",
    "    tree = copy.deepcopy(original_tree)\n",
    "    \n",
    "    #Remove cases like NP->DT, VP->NP\n",
    "    tree.collapse_unary(collapsePOS=True, collapseRoot=True)\n",
    "    #Convert to Chomsky\n",
    "    tree.chomsky_normal_form()\n",
    "    \n",
    "    tree = set_leave_lower(tree)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET TRAIN/TEST DATA\n",
    "cnf_train, cnf_test = get_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP pierre) (NNP vinken))\n",
      "    (NP-SBJ|<,-ADJP-,>\n",
      "      (, ,)\n",
      "      (NP-SBJ|<ADJP-,>\n",
      "        (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "        (, ,))))\n",
      "  (S|<VP-.>\n",
      "    (VP\n",
      "      (MD will)\n",
      "      (VP\n",
      "        (VB join)\n",
      "        (VP|<NP-PP-CLR-NP-TMP>\n",
      "          (NP (DT the) (NN board))\n",
      "          (VP|<PP-CLR-NP-TMP>\n",
      "            (PP-CLR\n",
      "              (IN as)\n",
      "              (NP\n",
      "                (DT a)\n",
      "                (NP|<JJ-NN> (JJ nonexecutive) (NN director))))\n",
      "            (NP-TMP (NNP nov.) (CD 29))))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "cnf_train[0].pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "To  better  understand  PCFG,  let’s  consider  the  first  parse  tree  in  the training data “cnf_train” as an example.  Run the code we have provided for you and then write down the roles of **.productions()**, **.rhs()**, **.lhs()**, **.leaves()** in the ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S -> NP-SBJ S|<VP-.>, NP-SBJ -> NP NP-SBJ|<,-ADJP-,>, NP -> NNP NNP, NNP -> 'pierre', NNP -> 'vinken', NP-SBJ|<,-ADJP-,> -> , NP-SBJ|<ADJP-,>, , -> ',', NP-SBJ|<ADJP-,> -> ADJP ,, ADJP -> NP JJ, NP -> CD NNS, CD -> '61', NNS -> 'years', JJ -> 'old', , -> ',', S|<VP-.> -> VP ., VP -> MD VP, MD -> 'will', VP -> VB VP|<NP-PP-CLR-NP-TMP>, VB -> 'join', VP|<NP-PP-CLR-NP-TMP> -> NP VP|<PP-CLR-NP-TMP>, NP -> DT NN, DT -> 'the', NN -> 'board', VP|<PP-CLR-NP-TMP> -> PP-CLR NP-TMP, PP-CLR -> IN NP, IN -> 'as', NP -> DT NP|<JJ-NN>, DT -> 'a', NP|<JJ-NN> -> JJ NN, JJ -> 'nonexecutive', NN -> 'director', NP-TMP -> NNP CD, NNP -> 'nov.', CD -> '29', . -> '.'] <class 'nltk.grammar.Production'>\n"
     ]
    }
   ],
   "source": [
    "rules = cnf_train[0].productions()\n",
    "print(rules, type(rules[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((NP-SBJ, S|<VP-.>), nltk.grammar.Nonterminal)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[0].rhs(), type(rules[0].rhs()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('61',), str)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[10].rhs(), type(rules[10].rhs()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(S, nltk.grammar.Nonterminal)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[0].lhs(), type(rules[0].lhs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pierre', 'vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "print(cnf_train[0].leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER HERE**\n",
    "- .productions(): The productions() method returns a list of the productions that correspond to the non-terminal symbol of the tree. These are the production rules when compacted according to Chomsky Normal Form.\n",
    "- .rhs(): The rhs() method returns the right-hand side of the production rule. This is the part of the production rule that is on the right side of the arrow i.e. either a terminal or non-terminal or both. So for example, if the production rule is A -> B C, then the right-hand side would be B C.\n",
    "- .lhs(): The lhs() method returns the left-hand side of the production rule. This is the part of the production rule that is on the left side of the arrow i.e. it must be a non-terminal symbol. So for example, if the production rule is A -> B C, then the left-hand side would be A. \n",
    "- .leaves(): The leaves() method returns a list of the terminal symbols in the tree. These are the symbols that do not have any children in the tree. So for example, if the tree is (A (B C)), then the leaves would be B and C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "To count the number of unique rules, nonterminals and terminals, please implement functions **collect_rules**, **collect_nonterminals**, **collect_terminals**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rules(train_data):\n",
    "    '''\n",
    "    Collect the rules that appear in data.\n",
    "    params:\n",
    "        train_data: list[Tree] --- list of Tree objects\n",
    "    return:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "        rules_counts: Counter object --- a dictionary that maps one rule (nltk.Nonterminal) to its number of \n",
    "                                         occurences (int) in train data.\n",
    "    '''\n",
    "    rules = list()\n",
    "    rule_counts = Counter()\n",
    "    ### YOUR CODE HERE\n",
    "    # Include rules that appear in the training data\n",
    "    for tree in train_data:\n",
    "        rules += tree.productions()\n",
    "    rule_counts = Counter(rules)\n",
    "    ### YOUR CODE HERE\n",
    "    return rules, rule_counts\n",
    "\n",
    "def collect_nonterminals(rules):\n",
    "    '''\n",
    "    collect nonterminals that appear in the rules\n",
    "    params:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "    return:\n",
    "        nonterminals: set(nltk.Nonterminal) --- set of nonterminals \n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    nonterminals = set()\n",
    "    for rule in rules:\n",
    "        nonterminals.add(rule.lhs())\n",
    "        for nonterminal in rule.rhs():\n",
    "            if isinstance(nonterminal, Nonterminal):\n",
    "                nonterminals.add(nonterminal)\n",
    "    ### END OF YOUR CODE\n",
    "    return nonterminals\n",
    "\n",
    "def collect_terminals(rules):\n",
    "    '''\n",
    "    collect terminals that appear in the rules\n",
    "    params:\n",
    "        rules: list[nltk.grammar.Production] --- list of rules (Production objects)\n",
    "    return:\n",
    "        terminals: set of strings --- set of terminals    \n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    terminals = set()\n",
    "    for rule in rules:\n",
    "        for terminal in rule.rhs():\n",
    "            if not isinstance(terminal, Nonterminal):\n",
    "                terminals.add(terminal)\n",
    "    ### END OF YOUR CODE\n",
    "    return terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rules, train_rules_counts = collect_rules(cnf_train)\n",
    "nonterminals = collect_nonterminals(train_rules)\n",
    "terminals = collect_terminals(train_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196646, 31656, 11367, 7869)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CORRECT ANSWER (19xxxx, 3xxxx, 1xxxx, 7xxx)\n",
    "len(train_rules), len(set(train_rules)), len(terminals), len(nonterminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(, -> ',', 4876), (DT -> 'the', 4726), (. -> '.', 3814), (PP -> IN NP, 3273), (S|<VP-.> -> VP ., 3003)]\n"
     ]
    }
   ],
   "source": [
    "print(train_rules_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Implement the function **build_pcfg** which builds a dictionary that stores the terminal rules and nonterminal rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pcfg(rules_counts):\n",
    "    '''\n",
    "    Build a dictionary that stores the terminal rules and nonterminal rules.\n",
    "    param:\n",
    "        rules_counts: Counter object --- a dictionary that maps one rule to its number of occurences in train data.\n",
    "    return:\n",
    "        rules_dict: dict(dict(dict)) --- a dictionary has a form like:\n",
    "                    rules_dict = {'terminals':{'NP':{'the':1000,'an':500}, 'ADJ':{'nice':500,'good':100}},\n",
    "                                  'nonterminals':{'S':{'NP@VP':1000},'NP':{'NP@NP':540}}}\n",
    "    When building \"rules_dict\", you need to use \"lhs()\", \"rhs()\" funtion and convert Nonterminal to str.\n",
    "    **All the keys in the dictionary are of type str**.\n",
    "    '@' is used as a special symbol to split left and right nonterminal strings.\n",
    "    '''\n",
    "    ### YOUR CODE HERE\n",
    "    rules_dict = dict()\n",
    "    rules_dict['terminals'], rules_dict['nonterminals'] = defaultdict(dict), defaultdict(dict)\n",
    "    for rule in rules_counts:\n",
    "        tmp = []\n",
    "        rhs = rule.rhs()\n",
    "        lhs = str(rule.lhs())\n",
    "        if len(rhs) == 1:\n",
    "            if type(rhs[0]) == str: # Assume that the terminal is a string\n",
    "                rules_dict['terminals'][lhs][rhs[0]] = rules_counts[rule]\n",
    "        else:        \n",
    "            for r in rhs: \n",
    "                tmp.append(str(r))\n",
    "            at_str = '@'.join(tmp)\n",
    "            rules_dict['nonterminals'][lhs][at_str] = rules_counts[rule]\n",
    "    ### END OF YOUR CODE\n",
    "    return rules_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rules_dict = build_pcfg(train_rules_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Find the terminal symbols in ''cnf_test[0]'' that never appeared in the PCFG we built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does\n",
      "exercise\n",
      "to\n",
      "leading\n",
      "the\n",
      "line-item\n",
      "have\n",
      "a\n",
      "two\n",
      "legal\n",
      "said\n",
      "*\n",
      "experts\n",
      "bush\n",
      "president\n",
      "0\n",
      "veto\n",
      "constitutional-law\n",
      "authority\n",
      "n't\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "test_terminals = collect_terminals(cnf_test[0].productions())\n",
    "for terminal in test_terminals:\n",
    "    if terminal not in train_rules_dict['terminals']:\n",
    "        print(terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "We can use smoothing techniques to handle these cases. A simple smoothing method is as follows. We first create a new \"unknown\" terminal symbol $unk$.\n",
    "\n",
    "Next, for each original non-terminal symbol $A\\in N$, we add one new rule $A \\rightarrow unk$ to the original PCFG.\n",
    "\n",
    "The smoothed probabilities for all rules can then be estimated as:\n",
    "$$q_{smooth}(A \\rightarrow \\beta) = \\frac {count(A \\rightarrow \\beta)}{count(A)+1}$$\n",
    "$$q_{smooth}(A \\rightarrow unk) = \\frac {1}{count(A)+1}$$\n",
    "where $|V|$ is the count of unique terminal symbols.\n",
    "\n",
    "\n",
    "Implement the function **smooth_rules_prob** which returns the smoothed rule probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_rules_prob(rules_counts):\n",
    "    '''\n",
    "    params:\n",
    "        rules_counts: dict(dict(dict)) --- a dictionary has a form like:\n",
    "                      rules_counts = {'terminals':{'NP':{'the':1000,'an':500}, 'ADJ':{'nice':500,'good':100}},\n",
    "                                      'nonterminals':{'S':{'NP@VP':1000},'NP':{'NP@NP':540}}}\n",
    "    \n",
    "    return:\n",
    "        rules_prob: dict(dict(dict)) --- a dictionary that has a form like:\n",
    "                               rules_prob = {'terminals':{'NP':{'the':0.6,'an':0.3, '<unk>':0.1},\n",
    "                                                          'ADJ':{'nice':0.6,'good':0.3,'<unk>':0.1},\n",
    "                                                          'S':{'<unk>':0.01}}}\n",
    "                                             'nonterminals':{'S':{'NP@VP':0.99}}\n",
    "    '''\n",
    "    rules_prob = copy.deepcopy(rules_counts)\n",
    "    unk = '<unk>'\n",
    "    ### YOUR CODE HERE\n",
    "    # if a nonterminal is queried from the terminal dictionary, return the probability of the unk token\n",
    "    for keys in rules_prob['terminals'].values():\n",
    "        sum = 0\n",
    "        for key_count in keys.values():\n",
    "            sum += key_count\n",
    "        for word, key_count in keys.items():\n",
    "            keys[word] = key_count / (sum + 1)\n",
    "        keys[unk] = 1 / (sum + 1)\n",
    "    for key, keys in rules_prob['nonterminals'].items():\n",
    "        sum = 0\n",
    "        for key_count in keys.values():\n",
    "            sum += key_count\n",
    "        for word, key_count in keys.items():\n",
    "            keys[word] = key_count / (sum + 1)\n",
    "        if key not in rules_prob['terminals']:\n",
    "            rules_prob['terminals'][key] = {unk:(1 / (sum + 1))}\n",
    "    ### END OF YOUR CODE\n",
    "    return rules_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_rules_prob = smooth_rules_prob(train_rules_dict)\n",
    "terminals.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1300172371337109\n",
      "0.025240088648116228\n",
      "0.039506305917861376\n",
      "{'<unk>': 5.389673385792821e-05}\n"
     ]
    }
   ],
   "source": [
    "print(s_rules_prob['nonterminals']['S']['NP-SBJ@S|<VP-.>'])\n",
    "print(s_rules_prob['nonterminals']['S']['NP-SBJ-1@S|<VP-.>'])\n",
    "print(s_rules_prob['nonterminals']['NP']['NNP@NNP'])\n",
    "print(s_rules_prob['terminals']['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11368"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "Estimate the probability of \"cnf_train[0]\", which is the first  parse  tree  in  the training data \"cnf_train\" by using \"s_rules_prob\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3824366894669981e-52\n"
     ]
    }
   ],
   "source": [
    "probability = 1\n",
    "\n",
    "# Probability of cnf_train[0] using s_rules_prob\n",
    "for rule in cnf_train[0].productions():\n",
    "    lhs = str(rule.lhs())\n",
    "    rhs = [str(r) for r in rule.rhs()]\n",
    "    if len(rhs) == 1:\n",
    "        if rhs[0] in s_rules_prob['terminals'][lhs]:\n",
    "            probability *= s_rules_prob['terminals'][lhs][rhs[0]]\n",
    "        else:\n",
    "            probability *= s_rules_prob['terminals'][lhs]['<unk>']\n",
    "    else:\n",
    "        at_str = '@'.join(rhs)\n",
    "        if at_str in s_rules_prob['nonterminals'][lhs]:\n",
    "            probability *= s_rules_prob['nonterminals'][lhs][at_str]\n",
    "        else:\n",
    "            probability *= s_rules_prob['terminals'][lhs]['<unk>']\n",
    "\n",
    "print(probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKY Algorithm\n",
    "\n",
    "Similar to the Viterbi algorithm, the CKY algorithm is a dynamic-programming algorithm. Given a PCFG $G=(N, \\ \\Sigma, \\ S, \\ R, \\ q)$, we can use the CKY algorithm described in class to find the highest scoring parse tree for a sentence. \n",
    "\n",
    "First, let us complete the *CKY* function from scratch using only Python built-in functions and the Numpy package. \n",
    "\n",
    "The output should be two dictionaries $\\pi$ and $bp$, which store the optimal probability and backpointer information respectively.\n",
    "\n",
    "Given a sentence $w_0, w_1, ...,w_{n-1}$,  $\\pi(i, k, X)$, $bp(i, k, X)$ refer to the highest score and backpointer for the (partial) parse tree that has the root X (a non-terminal symbol) and covers the word span $w_i, ..., w_{k-1}$, where $0 \\le i < k \\le n$. Note that a backpointer includes both the best grammar rule chosen and the best split point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Implement **CKY** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CKY(sent, rules_prob):\n",
    "    '''\n",
    "    params:\n",
    "        sent: list[str] --- a list of strings\n",
    "        rules_prob: dict(dict(dict)) --- a dictionary that has a form like:\n",
    "                                           rules_prob = {'terminals':{'NP':{'the':0.6,'an':0.3, '<unk>':0.1},\n",
    "                                                                      'ADJ':{'nice':0.6,'good':0.3,'<unk>':0.1},\n",
    "                                                                      'S':{'<unk>':0.01}}}\n",
    "                                                         'nonterminals':{'S':{'NP@VP':0.99}}\n",
    "    return:\n",
    "        score: dict(dict) --- score[(i,i+span)][root] represents the highest score for the parse (sub)tree that has the root \"root\"\n",
    "                          across words w_i, w_{i+1},..., w_{i+span-1}.\n",
    "        back: dict(dict) --- back[(i,i+span)][root] = (split , left_child, right_child); split: int; \n",
    "                         left_child: str; right_child: str. \n",
    "    '''\n",
    "    score = defaultdict(dict)\n",
    "    back = defaultdict(dict)\n",
    "    sent_len = len(sent)\n",
    "    ### YOUR CODE HERE\n",
    "    # CKY algorithm\n",
    "    for i in range(0, sent_len):\n",
    "        term = sent[i]\n",
    "        for A, dicts in rules_prob['terminals'].items():\n",
    "            if term in dicts:\n",
    "                score[(i, i+1)][A] = dicts[term]\n",
    "                back[(i, i+1)][A] = (-1, term, '')\n",
    "    \n",
    "    for span in range(2, sent_len + 1):  \n",
    "        for begin in range(0, sent_len - span + 1):\n",
    "            end = begin + span\n",
    "            \n",
    "            for split in range(begin + 1, end):\n",
    "                for A in rules_prob['nonterminals'].keys():                    \n",
    "                    for key in rules_prob['nonterminals'][A].keys():         \n",
    "                        B = key.split('@')[0]\n",
    "                        C = key.split('@')[1]  \n",
    "                        \n",
    "                        if (B in score[(begin, split)].keys()) and (C in score[(split, end)].keys()):\n",
    "                            updated_score = rules_prob['nonterminals'][A][key] *\\\n",
    "                                            score[(begin, split)][B] *\\\n",
    "                                            score[(split, end)][C]\n",
    "                            \n",
    "                            if (A not in score[(begin, end)]) or (updated_score > score[(begin, end)][A]):\n",
    "                                score[(begin, end)][A] = updated_score\n",
    "                                back[(begin, end)][A] = (split, B, C)\n",
    "\n",
    "    ### END OF YOUR CODE                  \n",
    "    return score, back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "What is the time complexity of CKY function (in worst case), please verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time complexity of the CKY Algorithm (in the worst case) is O(n^3 * |N|^2), where n is the length of the sentence and |N| is the number of non-terminal symbols in the grammar. This is because the algorithm iterates over all possible spans of the sentence and for each span, it iterates over all possible non-terminal symbols in the grammar. The algorithm also iterates over all possible splits of the span to find the best split point. This results in a time complexity of O(n^3 * |N|^2).\n",
    "\n",
    "Mathematically, this time complexity can be derived as follows:\n",
    "- The algorithm iterates over all possible spans of the sentence, which results in a time complexity of O(n^2).\n",
    "- For each span, the algorithm iterates over all possible non-terminal symbols in the grammar, which results in a time complexity of O(|N|).\n",
    "- For each non-terminal symbol, the algorithm iterates over all possible splits of the span to find the best split point, which results in a time complexity of O(n).\n",
    "- Combining these complexities, we get a total time complexity of O(n^3 * |N|^2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "Implement **build_tree** function to reconstruct the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(back, root):\n",
    "    '''\n",
    "    Build the tree recursively.\n",
    "    params:\n",
    "        back: dict() --- back[(i,i+span)][X] = (split , left_child, right_child); split:int; left_child: str; right_child: str.\n",
    "        root: tuple() --- (begin, end, nonterminal_symbol), e.g., (0, 10, 'S\n",
    "    return:\n",
    "        tree: nltk.tree.Tree\n",
    "    '''\n",
    "    begin = root[0]\n",
    "    end = root[1]\n",
    "    root_label = root[2]\n",
    "    ### YOUR CODE HERE\n",
    "    split, left, right = back[(begin, end)][root_label]\n",
    "    if right != '': # Assume that if right is not empty, then left is not empty\n",
    "        l_tree = build_tree(back, (begin, split, left))\n",
    "        r_tree = build_tree(back, (split, end, right))\n",
    "        \n",
    "        tree = nltk.tree.Tree(root_label, [l_tree, r_tree])\n",
    "    else:\n",
    "        tree = nltk.tree.Tree(root_label, [left])\n",
    "    ### END OF YOUR CODE \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "-  Given a parse tree, the original sentence can be obtained by arranging the terminal symbols from left to right. Use **CKY** function to compute the max probability for the sentence form \"cnf_train[0]\", which is the first parse tree in the training data \"cnf_train\".\n",
    "- Generate and display the parse tree of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rules, train_rules_counts = collect_rules(cnf_train)\n",
    "train_non, train_term = collect_nonterminals(train_rules), collect_terminals(train_rules)\n",
    "train_rules_dict = build_pcfg(train_rules_counts)\n",
    "train_rules_prob = smooth_rules_prob(train_rules_dict)\n",
    "train_term.add('<unk>')\n",
    "\n",
    "train_sent = cnf_train[0].leaves()\n",
    "score, back = CKY(train_sent, train_rules_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                         S                                                                                                    \n",
      "                         ________________________________________________|_____________________________                                                                        \n",
      "                        |                                                                           S|<VP-.>                                                                  \n",
      "                        |                                                 _____________________________|____________________________________________________________________   \n",
      "                        |                                                VP                                                                                                 | \n",
      "                        |                                            ____|_____________                                                                                     |  \n",
      "                        |                                           |                  VP                                                                                   | \n",
      "                        |                                           |     _____________|_______________                                                                     |  \n",
      "                      NP-SBJ                                        |    |                     VP|<NP-PP-CLR-NP-                                                            | \n",
      "                        |                                           |    |                            TMP>                                                                  | \n",
      "         _______________|___________                                |    |         ____________________|______________________                                              |  \n",
      "        |                    NP-SBJ|<,-NP-,>                        |    |        |                                     VP|<PP-CLR-NP-                                      | \n",
      "        |                           |                               |    |        |                                          TMP>                                           | \n",
      "        |           ________________|______________                 |    |        |                                 __________|___________________________________          |  \n",
      "        |          |                         NP-SBJ|<NP-,>          |    |        |                              PP-CLR                                           |         | \n",
      "        |          |                 ______________|____________    |    |        |          ______________________|__________                                    |         |  \n",
      "        |          |                NP                          |   |    |        |         |                                 NP                                  |         | \n",
      "        |          |     ___________|______________             |   |    |        |         |           ______________________|____________                       |         |  \n",
      "        NP         |    |                     NP|<NNS-JJ>       |   |    |        NP        |          |                               NP|<JJ-NN>               NP-TMP      | \n",
      "   _____|____      |    |            ______________|________    |   |    |     ___|____     |          |                       ____________|_________        _____|_____    |  \n",
      " NNP        NNP    ,    CD         NNS                      JJ  ,   MD   VB   DT       NN   IN         DT                     JJ                     NN    NNP          CD  . \n",
      "  |          |     |    |           |                       |   |   |    |    |        |    |          |                      |                      |      |           |   |  \n",
      "pierre     vinken  ,    61        years                    old  ,  will join the     board  as         a                 nonexecutive             director nov.         29  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tree = build_tree(back, (0, len(train_sent), 'S'))\n",
    "train_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "Run the remaining code to test your model on test data \"cnf_test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_leave_index(tree):\n",
    "    '''\n",
    "    Label the leaves of the tree with indexes\n",
    "    Arg:\n",
    "        tree: original tree, nltk.tree.Tree\n",
    "    Return:\n",
    "        tree: preprocessed tree, nltk.tree.Tree\n",
    "    '''\n",
    "    for idx, _ in enumerate(tree.leaves()):\n",
    "        tree_location = tree.leaf_treeposition(idx)\n",
    "        non_terminal = tree[tree_location[:-1]]\n",
    "        non_terminal[0] = non_terminal[0] + \"_\" + str(idx)\n",
    "    return tree\n",
    "\n",
    "def get_nonterminal_bracket(tree):\n",
    "    '''\n",
    "    Obtain the constituent brackets of a tree\n",
    "    Arg:\n",
    "        tree: tree, nltk.tree.Tree\n",
    "    Return:\n",
    "        nonterminal_brackets: constituent brackets, set\n",
    "    '''\n",
    "    nonterminal_brackets = set()\n",
    "    for tr in tree.subtrees():\n",
    "        label = tr.label()\n",
    "        #print(tr.leaves())\n",
    "        if len(tr.leaves()) == 0:\n",
    "            continue\n",
    "        start = tr.leaves()[0].split('_')[-1]\n",
    "        end = tr.leaves()[-1].split('_')[-1]\n",
    "        if start != end:\n",
    "            nonterminal_brackets.add(label+'-('+start+':'+end+')')\n",
    "    return nonterminal_brackets\n",
    "\n",
    "def word2lower(w, terminals):\n",
    "    '''\n",
    "    Map an unknow word to \"unk\"\n",
    "    '''\n",
    "    return w.lower() if w in terminals else '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Test Tree: 1\n",
      "Constituent number in the predicted tree: 20\n",
      "Constituent number in the gold tree: 20\n",
      "Correct constituent number: 14\n",
      "####################\n",
      "Test Tree: 2\n",
      "Constituent number in the predicted tree: 54\n",
      "Constituent number in the gold tree: 54\n",
      "Correct constituent number: 30\n",
      "####################\n",
      "Test Tree: 3\n",
      "Constituent number in the predicted tree: 30\n",
      "Constituent number in the gold tree: 30\n",
      "Correct constituent number: 23\n",
      "####################\n",
      "Test Tree: 4\n",
      "Constituent number in the predicted tree: 17\n",
      "Constituent number in the gold tree: 17\n",
      "Correct constituent number: 16\n",
      "####################\n",
      "Test Tree: 5\n",
      "Constituent number in the predicted tree: 32\n",
      "Constituent number in the gold tree: 32\n",
      "Correct constituent number: 26\n",
      "####################\n",
      "Test Tree: 6\n",
      "Constituent number in the predicted tree: 40\n",
      "Constituent number in the gold tree: 40\n",
      "Correct constituent number: 18\n",
      "####################\n",
      "Test Tree: 7\n",
      "Constituent number in the predicted tree: 22\n",
      "Constituent number in the gold tree: 22\n",
      "Correct constituent number: 7\n",
      "####################\n",
      "Test Tree: 8\n",
      "Constituent number in the predicted tree: 18\n",
      "Constituent number in the gold tree: 18\n",
      "Correct constituent number: 6\n",
      "####################\n",
      "Test Tree: 9\n",
      "Constituent number in the predicted tree: 28\n",
      "Constituent number in the gold tree: 28\n",
      "Correct constituent number: 16\n",
      "####################\n",
      "Test Tree: 10\n",
      "Constituent number in the predicted tree: 40\n",
      "Constituent number in the gold tree: 40\n",
      "Correct constituent number: 8\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "pred_count = 0\n",
    "gold_count = 0\n",
    "for i, t in enumerate(cnf_test):\n",
    "    #Protect the original tree \n",
    "    t = copy.deepcopy(t)\n",
    "    sent = t.leaves()  \n",
    "    #Map the unknow words to \"unk\"\n",
    "    sent = [word2lower(w.lower(), terminals) for w in sent]\n",
    "    \n",
    "    #CKY algorithm\n",
    "    score, back = CKY(sent, s_rules_prob)\n",
    "    candidate_tree = build_tree(back, (0, len(sent), 'S'))\n",
    "    \n",
    "    #Extract constituents from the gold tree and predicted tree\n",
    "    pred_tree = set_leave_index(candidate_tree)\n",
    "    pred_brackets = get_nonterminal_bracket(pred_tree)\n",
    "    \n",
    "    #Count correct constituents\n",
    "    pred_count += len(pred_brackets)\n",
    "    gold_tree = set_leave_index(t)\n",
    "    gold_brackets = get_nonterminal_bracket(gold_tree)\n",
    "    gold_count += len(gold_brackets)\n",
    "    current_correct_num = len(pred_brackets.intersection(gold_brackets))\n",
    "    correct_count += current_correct_num\n",
    "    \n",
    "    print('#'*20)\n",
    "    print('Test Tree:', i+1)\n",
    "    print('Constituent number in the predicted tree:', len(pred_brackets))\n",
    "    print('Constituent number in the gold tree:', len(gold_brackets))\n",
    "    print('Correct constituent number:', current_correct_num)\n",
    "\n",
    "recall = correct_count/gold_count\n",
    "precision = correct_count/pred_count\n",
    "f1 = 2*recall*precision/(recall+precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall precision: 0.545, recall: 0.545, f1: 0.545\n"
     ]
    }
   ],
   "source": [
    "print('Overall precision: {:.3f}, recall: {:.3f}, f1: {:.3f}'.format(precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
