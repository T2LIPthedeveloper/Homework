{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIRhc7drd8AY"
   },
   "source": [
    "# 50.040 Natural Language Processing (Fall 2024) Mini Project (40 Points)\n",
    "\n",
    "**DUE DATE: 25 October 2024**\n",
    "\n",
    "This homework will be graded by Chen Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsCU5Q_6d8Ac"
   },
   "source": [
    "\n",
    "# Personal Information (Fill before you start)\n",
    "\n",
    "**STUDENT ID:** 1006184\n",
    "\n",
    "**Name:** Atul Parida\n",
    "\n",
    "**Students with whom you have discussed (if any):**\n",
    "\n",
    "**Please also rename the final submitted pdf as ``miniproject_[NAME]_[STUDENTID].pdf``**\n",
    "\n",
    "**-1 points if info not filled or file name not adjusted before submission, -40 points if you copy other's answer. We encourage discussion, but please do not copy without thinking.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDOUuC7-CdX-"
   },
   "source": [
    "## [!] Please read this if your computer does not have GPUs.\n",
    "### Free GPU Resources\n",
    "We suggest that you run neural language models on machines with GPU(s). Google provides the free online platform [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb), a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use as common packages have been  pre-installed. Google users can have access to a Tesla T4 GPU (approximately 15G memory). Note that when you connect to a GPU-based VM runtime, you are given a maximum of 12 hours at a time on the VM.\n",
    "\n",
    "Colab is web-based, fast and convinient. You can simply upload this notebook and run it online. For the database needed in this task, you can download it and upload to colab OR you can save it in your google drive and link it with the colab.\n",
    "\n",
    "It is convenient to upload local Jupyter Notebook files and data to Colab, please refer to the [tutorial](https://colab.research.google.com/notebooks/io.ipynb). \n",
    "\n",
    "In addition, Microsoft also provides the online platform [Azure Notebooks](https://notebooks.azure.com/help/introduction) for research of data science and machine learning, there are free trials for new users with credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0iF5uWcCdXV"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Language models are very useful for a wide range of applications, e.g., speech recognition and machine translation. Consider a sentence consisting of words $x_1, x_2, …, x_m$, where $m$ is the length of the sentence, the goal of language modeling is to model the probability of the sentence, where $m \\geq 1$, $x_i \\in V $ and $V$ is the vocabulary of the corpus:\n",
    "$$p(x_1, x_2, …, x_m)$$\n",
    "In this project, we are going to explore both statistical language model and neural language model on the [Wikitext-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) datasets. Download wikitext-2 word-level data and put it under the ``data`` folder. There should be 3 data files in total: ``wiki.train.tokens``, ``wiki.valid.tokens`` and ``wiki.test.tokens``.\n",
    "\n",
    "<font color=red> **(Please download the dataset before you proceed. Contact TA if you have trouble in this step.)** </font>\n",
    "\n",
    "## Statistical  Language Model\n",
    "\n",
    "A simple way is to view words as independent random variables (i.e., zero-th order Markovian assumption). The joint probability can be written as:\n",
    "$$p(x_1, x_2, …, x_m)=\\prod_{i=1}^m p(x_i)$$\n",
    "However, this model ignores the word order information, to account for which, under the first-order Markovian assumption, the joint probability can be written as:\n",
    "$$p(x_0, x_1, x_2, …, x_{m})= \\prod_{i=1}^{m}p(x_i \\mid x_{i-1})$$\n",
    "Under the second-order Markovian assumption, the joint probability can be written as:\n",
    "$$p(x_{-1}, x_0, x_1, x_2, …, x_{m})= \\prod_{i=1}^{m}p(x_i \\mid x_{i-2}, x_{i-1})$$\n",
    "Similar to what we did in HMM, we will assume that $x_{-1}=START, x_0=START, x_{m} = STOP$ in this definition, where $START, STOP$ are special symbols referring to the start and the end of a sentence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5wC4lqKCdXW"
   },
   "source": [
    "### Parameter estimation\n",
    "\n",
    "Let's use $count(u)$ to denote the number of times the unigram $u$ appears in the corpus, use $count(v, u)$ to denote the number of times the bigram $v, u$ appears in the corpus, and $count(w, v, u)$ the times the trigram $w, v, u$ appears in the corpus, $u \\in V \\cup STOP$ and $w, v \\in V \\cup START$.\n",
    "\n",
    "And the parameters of the unigram, bigram and trigram models can be obtained using maximum likelihood estimation (MLE).\n",
    "\n",
    "- In the unigram model, the parameters can be estimated as: $$p(u) = \\frac {count(u)}{c}$$, where $c$ is the total number of words in the corpus.\n",
    "- In the bigram model, the parameters can be estimated as:\n",
    "$$p(u \\mid v) = \\frac{count(v, u)}{count(v)}$$\n",
    "- In the trigram model, the parameters can be estimated as:\n",
    "$$p(u \\mid w, v) = \\frac{count(w, v, u)}{count(w, v)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7dks8t0CdXW"
   },
   "source": [
    "### Smoothing the parameters\n",
    "#### Add-k Smoothing\n",
    "Note, it is likely that many parameters of bigram and trigram models will be 0 because the relevant bigrams and trigrams involved do not appear in the corpus. If you don't have a way to handle these 0 probabilities, all the sentences that include such bigrams or trigrams will have probabilities of 0.\n",
    "\n",
    "We'll use a Add-k Smoothing method to fix this problem, the smoothed parameters can be estimated as:\n",
    "\n",
    "$$p_{add-k}(u)= \\frac{count(u)+k}{c+k|V^*|}$$\n",
    "\n",
    "$$p_{add-k}(u \\mid v)= \\frac{count(v, u)+k}{count(v)+k|V^*|}$$\n",
    "\n",
    "$$p_{add-k}(u \\mid w, v)= \\frac{count(w, v, u)+k}{count(w, v)+k|V^*|}$$\n",
    "\n",
    "where $k \\in (0, 1)$ is the parameter of this approach, and $|V^*|$ is the size of the vocabulary $V^*$, here $V^*= V \\cup STOP$. One way to choose the value of $k$ is by\n",
    "optimizing the perplexity of the development set, namely to choose the value that minimizes the perplexity.\n",
    "#### Interpolation\n",
    "There is another way for smoothing which is named as **interpolation**. In interpolation, we always mix the probability estimates from\n",
    "all the n-gram estimators, weighing and combining the trigram, bigram, and unigram counts. In simple linear interpolation, we combine different order n-grams by linearly interpolating all the models. Thus, we estimate the trigram probability $p(w_n|w_{n-2},w_{n-1})$ by mixing together the unigram, bigram, and trigram probabilities, each weighted by a $\\lambda$:\n",
    "$$\\hat{p}(w_n|w_{n-2},w_{n-1}) = \\lambda_1p(w_n|w_{n-2},w_{n-1})+\\lambda_2p(w_n|w_{n-1})+\\lambda_3p(w_n)$$\n",
    "such that the $\\lambda_s$ sum to 1:\n",
    "$$\\sum_i\\lambda_i=1$$\n",
    "In addition, $\\lambda_1,\\lambda_2,\\lambda_3\\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNVha_8UCdXX"
   },
   "source": [
    "### Perplexity\n",
    "\n",
    "Given a test set $D^{\\prime}$ consisting of sentences $X^{(1)}, X^{(2)}, …, X^{(|D^{\\prime}|)}$, each sentence $X^{(j)}$ consists of words $x_1^{(j)}, x_2^{(j)},…,x_{n_j}^{(j)}$, we can measure the probability of each sentence $X^{(j)}$, and the quality of the language model would be the probability it assigns to the entire set of test sentences, namely:\n",
    "\\begin{equation} \n",
    "\\prod_{j=1}^{|D^{\\prime}|}p(X^{(j)})\n",
    "\\end{equation}\n",
    "Let's define average $log_2$ probability as:\n",
    "\\begin{equation} \n",
    "l=\\frac{1}{c^{\\prime}}\\sum_{j=1}^{|D^{\\prime}|}log_2p(X^{(j)})\n",
    "\\end{equation}\n",
    "$c^{\\prime}$ is the total number of words in the test set, $|D^{\\prime}|$ is the number of sentences. And the perplexity is defined as:\n",
    "\\begin{equation} \n",
    "perplexity=2^{-l}\n",
    "\\end{equation}\n",
    "\n",
    "The lower the perplexity, the better the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.8.1+cu111 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (1.8.1+cu111)\n",
      "Requirement already satisfied: torchvision==0.9.1+cu111 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (0.9.1+cu111)\n",
      "Requirement already satisfied: torchaudio==0.8.1 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: torchtext==0.9.1 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from torch==1.8.1+cu111) (4.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from torch==1.8.1+cu111) (1.24.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from torchvision==0.9.1+cu111) (10.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from torchtext==0.9.1) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from torchtext==0.9.1) (2.32.3)\n",
      "Requirement already satisfied: click in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests->torchtext==0.9.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests->torchtext==0.9.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests->torchtext==0.9.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages (from requests->torchtext==0.9.1) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html torchtext==0.9.1 nltk\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wikitext-2/wiki.train.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    train_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    train_sents = [s for s in train_sents if len(s)>0 and s[0] != '=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the game began development in 2010 , carrying over a\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(train_sents[1][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atulp\\anaconda3\\envs\\testenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Test CUDA 11.1 working\n",
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0vxL-WpCdXX"
   },
   "source": [
    "### Question 1.1 [code] **(4 points)** \n",
    "Implement the function **\"compute_ngram\"** that computes n-grams in the corpus.\n",
    " (Do not take the START and STOP symbols into consideration.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOyoT-QiCdXY"
   },
   "outputs": [],
   "source": [
    "def compute_ngram(sents, n):\n",
    "    '''\n",
    "    Compute n-grams that appear in \"sents\".\n",
    "    param:\n",
    "        sents: list[list[str]] --- list of list of word strings\n",
    "        n: int --- \"n\" gram\n",
    "    return:\n",
    "        ngram_set: set{str} --- a set of n-grams (no duplicate elements)\n",
    "        ngram_dict: dict{ngram: counts} --- a dictionary that maps each ngram to its number occurence in \"sents\";\n",
    "        This dict contains the parameters of our ngram model. E.g. if n=2, ngram_dict={('a','b'):10, ('b','c'):13}\n",
    "        \n",
    "        You may need to use \"Counter\", \"tuple\" function here.\n",
    "    '''\n",
    "    ngram_set = None\n",
    "    ngram_dict = None\n",
    "    ### YOUR CODE HERE\n",
    "    ngram_dict = Counter()\n",
    "\n",
    "    for sent in sents:\n",
    "        ngram_dict.update([tuple(sent[i:i+n]) for i in range(len(sent)-n+1)])\n",
    "\n",
    "    ngram_set = set(ngram_dict.keys())\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return ngram_set, ngram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram: 28910\n",
      "bigram: 577343\n",
      "trigram: 1344047\n"
     ]
    }
   ],
   "source": [
    "unigram_set, unigram_dict = compute_ngram(train_sents, 1)\n",
    "print('unigram: %d' %(len(unigram_set)))\n",
    "bigram_set, bigram_dict = compute_ngram(train_sents, 2)\n",
    "print('bigram: %d' %(len(bigram_set)))\n",
    "trigram_set, trigram_dict = compute_ngram(train_sents, 3)\n",
    "print('trigram: %d' %(len(trigram_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 [code] **(2 points)** \n",
    "List 5 most frequent unigrams, bigrams and trigrams as well as their counts.(Hint: use the built-in function .most_common in Counter class)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most frequent unigrams:\n",
      "[(('the',), 130519), ((',',), 99763), (('.',), 73388), (('of',), 56743), (('<unk>',), 53951)]\n",
      "5 most frequent bigrams:\n",
      "[(('of', 'the'), 17242), (('in', 'the'), 11778), ((',', 'and'), 11643), (('.', 'the'), 11274), ((',', 'the'), 8024)]\n",
      "5 most frequent trigrams:\n",
      "[((',', 'and', 'the'), 1393), ((',', '<unk>', ','), 950), (('<unk>', ',', '<unk>'), 901), (('one', 'of', 'the'), 866), (('<unk>', ',', 'and'), 819)]\n"
     ]
    }
   ],
   "source": [
    "# List 5 most frequent unigrams, bigrams and trigrams as well as their counts.\n",
    "### YOUR CODE HERE\n",
    "print('5 most frequent unigrams:')\n",
    "print(unigram_dict.most_common(5))\n",
    "print('5 most frequent bigrams:')\n",
    "print(bigram_dict.most_common(5))\n",
    "print('5 most frequent trigrams:')\n",
    "print(trigram_dict.most_common(5))\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVpoDgU7CdXb"
   },
   "source": [
    "### Question 2 [code] **(4 points)**\n",
    "Now, we take the START and STOP symbols into consideration. So we need to pad the **train_sents** as described in \"Statistical Language Model\" before we apply \"compute_ngram\" function. For example, given a sentence \"I like NLP\", in a bigram model, we need to pad it as \"START I like NLP STOP\", in a trigram model, we need to pad it as \"START START I like NLP STOP\". For unigram model, it should be paded as \"I like NLP STOP\".\n",
    "\n",
    "1. Implement the ``pad_sents`` function.\n",
    "2. Pad ``train_sents``.\n",
    "3. Apply ``compute_ngram`` function to these padded sents.\n",
    "4. Implement ``ngram_prob`` function. Compute the probability for each n-gram in the variable **ngrams** according equations in **\"Parameter estimation\"**. List down the n-grams that have 0 probability. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the'], ['the', 'computer'], ['go'], ['go', 'to'], ['have'], ['have', 'had'], ['and', 'the'], ['can'], ['can', 'sea'], ['a', 'number', 'of'], ['with', 'respect', 'to'], ['in', 'terms', 'of'], ['not', 'good', 'bad']]\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "ngrams = list()\n",
    "with open('data/ngram.txt','r') as f:\n",
    "    for line in f:\n",
    "        ngrams.append(line.strip('\\n').split())\n",
    "print(ngrams)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '<START>'\n",
    "STOP = '<STOP>'\n",
    "###################################\n",
    "def pad_sents(sents, n):\n",
    "    '''\n",
    "    Pad the sents according to n.\n",
    "    params:\n",
    "        sents: list[list[str]] --- list of sentences.\n",
    "        n: int --- specify the padding type, 1-gram, 2-gram, or 3-gram.\n",
    "    return:\n",
    "        padded_sents: list[list[str]] --- list of padded sentences.\n",
    "    '''\n",
    "    #padded_sents = None\n",
    "    padded_sents = []\n",
    "    ### YOUR CODE HERE\n",
    "    for sent in sents:\n",
    "        padded_sent = [START] * (n - 1) + sent + [STOP]\n",
    "        padded_sents.append(padded_sent)\n",
    "    ### END OF YOUR CODE\n",
    "    return padded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_sents = pad_sents(train_sents, 1)\n",
    "bi_sents = pad_sents(train_sents, 2)\n",
    "tri_sents = pad_sents(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_set, unigram_dict = compute_ngram(uni_sents, 1)\n",
    "bigram_set, bigram_dict = compute_ngram(bi_sents, 2)\n",
    "trigram_set, trigram_dict = compute_ngram(tri_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28911, 580825, 1363266)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_set),len(bigram_set),len(trigram_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024702\n"
     ]
    }
   ],
   "source": [
    "num_words = sum([v for _,v in unigram_dict.items()])\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_prob(ngram, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
    "    '''\n",
    "    params:\n",
    "        ngram: list[str] --- a list that represents n-gram\n",
    "        num_words: int --- total number of words\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        prob: float --- probability of the \"ngram\"\n",
    "    '''\n",
    "    prob = None\n",
    "    ### YOUR CODE HERE\n",
    "    try:\n",
    "        if len(ngram) == 1:\n",
    "            prob = unigram_dic[tuple(ngram)] / num_words\n",
    "        elif len(ngram) == 2:\n",
    "            prob = bigram_dic[tuple(ngram)] / unigram_dic[tuple(ngram[:1])]\n",
    "        elif len(ngram) == 3:\n",
    "            prob = trigram_dic[tuple(ngram)] / bigram_dic[tuple(ngram[:2])]\n",
    "    except ZeroDivisionError:\n",
    "        prob = 0.0\n",
    "    ### END OF YOUR CODE\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06446331361355893"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_prob(ngrams[0], num_words,unigram_dict, bigram_dict, trigram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['can', 'sea'], ['not', 'good', 'bad']]\n"
     ]
    }
   ],
   "source": [
    "### List down the n-grams that have 0 probability. \n",
    "### YOUR CODE HERE\n",
    "zero_prob_ngrams = []\n",
    "for ngram in ngrams:\n",
    "    if ngram_prob(ngram, num_words, unigram_dict, bigram_dict, trigram_dict) == 0.0:\n",
    "        zero_prob_ngrams.append(ngram)\n",
    "print(zero_prob_ngrams)\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kAezpJ9CdXd"
   },
   "source": [
    "### Question 3 [code] **(4 points)**\n",
    "\n",
    "1. Implement ``add_k_smoothing_ngram`` function to estimate ngram probability with ``add-k`` smoothing technique.\n",
    "2. Implement ``interpolation_ngram`` function to estimate ngram probability with ``interpolation`` smoothing technique.\n",
    "2. Implement ``perplexity`` function to compute the perplexity of the corpus \"**valid_sents**\" according to \"**Perplexity**\" section. The computation of $p(X^{(j)})$ depends on the n-gram model you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wikitext-2/wiki.valid.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    valid_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    valid_sents = [s for s in valid_sents if len(s)>0 and s[0] != '=']\n",
    "\n",
    "uni_valid_sents = pad_sents(valid_sents, 1)\n",
    "bi_valid_sents = pad_sents(valid_sents, 2)\n",
    "tri_valid_sents = pad_sents(valid_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_k_smoothing_ngram(ngram, k, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
    "    '''\n",
    "    params:\n",
    "        ngram: list[str] --- a list that represents n-gram\n",
    "        k: float \n",
    "        num_words: int --- total number of words\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        s_prob: float --- probability of the \"ngram\"\n",
    "    '''\n",
    "    s_prob = None\n",
    "    V = len(unigram_dic)\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Use the formula for add-k smoothing\n",
    "    def add_k(n, d, k, V):\n",
    "        return (n + k) / (d + k * V)\n",
    "    \n",
    "    try:\n",
    "        if len(ngram) == 1:\n",
    "            s_prob = add_k(unigram_dic[tuple(ngram)], num_words, k, V)\n",
    "        elif len(ngram) == 2:\n",
    "            s_prob = add_k(bigram_dic[tuple(ngram)], unigram_dic[tuple(ngram[:1])], k, V)\n",
    "        elif len(ngram) == 3:\n",
    "            s_prob = add_k(trigram_dic[tuple(ngram)], bigram_dic[tuple(ngram[:2])], k, V)        \n",
    "    except:\n",
    "        n = 0\n",
    "        d = 0\n",
    "    ### END OF YOUR CODE\n",
    "    return s_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_ngram(ngram, lam, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
    "    '''\n",
    "    params:\n",
    "        ngram: list[str] --- a list that represents n-gram\n",
    "        lam: list[float] --- a list of length 3.lam[0], lam[1] and lam[2] are correspondence to trigram, bigram and unigram,repectively.\n",
    "                             If len(ngram) == 1, lam[0]=lam[1]=0, lam[2]=1. If len(ngram) == 2, lam[0]=0. lam[0]+lam[1]+lam[2] = 1.\n",
    "        num_words: int --- total number of words\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        s_prob: float --- probability of the \"ngram\"\n",
    "    '''\n",
    "    s_prob = None\n",
    "    ### YOUR CODE HERE\n",
    "    if len(ngram) == 1:\n",
    "        lam[0] = lam[1] = 0\n",
    "        lam[2] = 1\n",
    "        s_prob = lam[2] * ngram_prob(ngram, num_words, unigram_dic, bigram_dic, trigram_dic)\n",
    "    elif len(ngram) == 2:\n",
    "        lam[0] = 0\n",
    "        s_prob = lam[1] * ngram_prob(ngram, num_words, unigram_dic, bigram_dic, trigram_dic) + lam[2] * ngram_prob(ngram[1:], num_words, unigram_dic, bigram_dic, trigram_dic)\n",
    "    elif len(ngram) == 3:\n",
    "        # Proportionately normalise the lambdas\n",
    "        lam = [l / sum(lam) for l in lam]\n",
    "        s_prob = lam[0] * ngram_prob(ngram, num_words, unigram_dic, bigram_dic, trigram_dic) + lam[1] * ngram_prob(ngram[1:], num_words, unigram_dic, bigram_dic, trigram_dic) + lam[2] * ngram_prob(ngram[2:], num_words, unigram_dic, bigram_dic, trigram_dic)\n",
    "    ### END OF YOUR CODE\n",
    "    return s_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'had']\n",
      "0.011710673716309527 0.004086999802140221\n"
     ]
    }
   ],
   "source": [
    "add_k_prob = add_k_smoothing_ngram(ngrams[5], 0.01, num_words, unigram_dict, bigram_dict, trigram_dict)\n",
    "interpolation_prob = interpolation_ngram(ngrams[5], [0.6,0.3,0.1], num_words, unigram_dict, bigram_dict, trigram_dict)\n",
    "print(ngrams[5])\n",
    "print(add_k_prob, interpolation_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(n, method, num_words, valid_sents, unigram_dic, bigram_dic, trigram_dic, k=0, lam=[0,0,1]):\n",
    "#     params:\n",
    "#         n: int --- n-gram model you choose \n",
    "#         method: int ---- method == 0, use add_k_smoothing; method != 0, use interpolation method.\n",
    "#         num_words: int --- total number of words\n",
    "#         valid_sents: list[list[str]] --- list of sentences\n",
    "#         unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "#         bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "#         trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "#         k: float --- The parameter of add_k_smoothing\n",
    "#         lam: list[float] --- a list of length 3. The parameter of interpolation. \n",
    "#    return:\n",
    "#         ppl: float --- perplexity of valid_sents\n",
    "#     '''\n",
    "#     '''\n",
    "    ppl = None\n",
    "    ### YOUR CODE HERE\n",
    "    prob = 0\n",
    "    if method == 0:\n",
    "      for i in valid_sents:\n",
    "        start=0\n",
    "        for j in range(n,len(i)):\n",
    "            prob+=np.log2(add_k_smoothing_ngram(i[start:j], k, num_words, unigram_dict, bigram_dict, trigram_dict))\n",
    "            start+=1\n",
    "            # Handle div by 0\n",
    "            if prob == float('-inf'):\n",
    "                prob = 0\n",
    "    else:\n",
    "      for i in valid_sents:\n",
    "        start = 0\n",
    "        for j in range(n,len(i)):\n",
    "            prob += np.log2(interpolation_ngram(i[start:j], lam, num_words, unigram_dict, bigram_dict, trigram_dict))\n",
    "            start+=1\n",
    "            # Handle div by 0\n",
    "            if prob == float('-inf'):\n",
    "                prob = 0\n",
    "    ppl=2**(-1*prob/np.sum([len(i) for i in valid_sents]))\n",
    "    ### END OF YOUR CODE\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799.6619443069386"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(1, 0, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict, k=0.1, lam=[0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799.6619443069386"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(1, 0, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict, k=0.1, lam=[0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 [code][written] **(4 points)**\n",
    "1. Based on add-k smoothing method, try out different $k\\in [ 0.0001, 0.001, 0.01, 0.1, 0.5]$ and different n-gram model (unigram, bigram and trigram). Find the model and $k$ that gives the best perplexity on \"**valid_sents**\" (smaller is better).\n",
    "2. Based on interpolation method, try out different $\\lambda$ where $\\lambda_1 = \\lambda_2$ and $\\lambda_3\\in [0.1, 0.2, 0.4, 0.6, 0.8]$. Find the $\\lambda$ that gives the best perplexity on \"**valid_sents**\" (smaller is better).\n",
    "3. Based on the methods and parameters we provide, choose the method that peforms best on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best values so far:  799.4969893037119 {'n': 1, 'k': 0.0001}\n",
      "Best values so far:  660.191107247087 {'n': 2, 'k': 0.0001}\n",
      "Best values so far:  481.1084956934896 {'n': 2, 'k': 0.001}\n",
      "Best values so far:  478.36441587832854 {'n': 2, 'k': 0.01}\n",
      "478.36441587832854 {'n': 2, 'k': 0.01}\n"
     ]
    }
   ],
   "source": [
    "n = [1,2,3]\n",
    "k = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "\n",
    "best_ppl = float('inf')\n",
    "best_nk = {'n':0,'k':0}\n",
    "### YOUR CODE HERE (add-k smoothing method)\n",
    "\n",
    "for n_val in n:\n",
    "    for k_val in k:\n",
    "        ppl = perplexity(n_val, 0, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict, k_val, lam=[0,0,1])\n",
    "        if ppl < best_ppl:\n",
    "            best_ppl = ppl\n",
    "            best_nk = {'n': n_val, 'k': k_val}\n",
    "            print(\"Best values so far: \", best_ppl, best_nk)\n",
    "\n",
    "### END OF YOUR CODE\n",
    "print(best_ppl, best_nk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best values so far:  312.40888050539996 {'lambda': 0.1}\n",
      "Best values so far:  282.2676598367113 {'lambda': 0.2}\n",
      "Best values so far:  271.68061888232074 {'lambda': 0.4}\n",
      "271.68061888232074 {'lambda': 0.4}\n"
     ]
    }
   ],
   "source": [
    "lambda_3 = [0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "best_ppl = float('inf')\n",
    "best_nk = {'lambda':0}\n",
    "### YOUR CODE HERE (interpolation method)\n",
    "\n",
    "for lam_val in lambda_3:\n",
    "    ppl = perplexity(3, 1, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict, k=0.1, lam=[(1 - lam_val) / 2,(1 - lam_val) / 2, lam_val])\n",
    "    if ppl <= best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_nk = {'lambda': lam_val}\n",
    "        print(\"Best values so far: \", best_ppl, best_nk)\n",
    "\n",
    "### END OF YOUR CODE\n",
    "print(best_ppl, best_nk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the methods and parameters we provide, choose the method that peforms best on the validation data (**write your answer**): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is the one with the lowest perplexity on the validation data. In this case, it is the trigram model using interpolation smoothing, with lambda being 0.4. I suspect this is because it's the closest to having equal weightages for the unigram, bigram and trigram models when the third lambda value equals 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3o_yjh9lCdXg"
   },
   "source": [
    "### Question 5 [code] **(4 points)**\n",
    "\n",
    "Evaluate the perplexity of the test data **test_sents** based on the best model you choose in **Question 4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7hGcXgCCdXg"
   },
   "outputs": [],
   "source": [
    "with open('data/wikitext-2/wiki.test.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    test_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    test_sents = [s for s in test_sents if len(s)>0 and s[0] != '=']\n",
    "\n",
    "uni_test_sents = pad_sents(test_sents, 1)\n",
    "bi_test_sents = pad_sents(test_sents, 2)\n",
    "tri_test_sents = pad_sents(test_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on test set with unigram:  255.223922904202\n",
      "Perplexity on test set with bigram:  256.294503525543\n",
      "Perplexity on test set with trigram:  259.35022996725246\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "# Ideal lambda value is 0.4\n",
    "lam_val = 0.4\n",
    "\n",
    "ppl = perplexity(3, 1, num_words, uni_test_sents, unigram_dict, bigram_dict, trigram_dict, k=0.1, lam=[(1 - lam_val) / 2,(1 - lam_val) / 2, lam_val])\n",
    "print(\"Perplexity on test set with unigram: \", ppl)\n",
    "ppl = perplexity(3, 1, num_words, bi_test_sents, unigram_dict, bigram_dict, trigram_dict, k=0.1, lam=[(1 - lam_val) / 2,(1 - lam_val) / 2, lam_val])\n",
    "print(\"Perplexity on test set with bigram: \", ppl)\n",
    "ppl = perplexity(3, 1, num_words, tri_test_sents, unigram_dict, bigram_dict, trigram_dict, k=0.1, lam=[(1 - lam_val) / 2,(1 - lam_val) / 2, lam_val])\n",
    "print(\"Perplexity on test set with trigram: \", ppl)\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePSI8RDWCdXj"
   },
   "source": [
    "## Neural Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkoTco_jCdXj"
   },
   "source": [
    "<img src=\"bilstm.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "\n",
    "We will create a LSTM language model as shown in figure and train it on the Wikitext-2 dataset. \n",
    "The data generators (train\\_iter, valid\\_iter, test\\_iter) have been provided. \n",
    "The word embeddings together with the parameters in the LSTM model will be learned from scratch.\n",
    "\n",
    "[Pytorch](https://pytorch.org/tutorials/) and [torchtext](https://torchtext.readthedocs.io/en/latest/index.html#) are required in this part. Do not make any changes to the provided code unless you are requested to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 [code] **(10 points)**\n",
    "- Implement the ``__init__`` function in ``LangModel`` class.\n",
    "- Implement the ``forward`` function in ``LangModel`` class.\n",
    "- Complete the training code in ``train`` function.\n",
    "    Then complete the testing code in  ``test`` function and \n",
    "    compute the perplexity of the test data ``test_iter``. The test perplexity should be below 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tF2GCRdFCdXk",
    "outputId": "7e2ee790-c23f-48dd-e4a2-763293db1880"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atulp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\atulp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fbc6e2e9d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy.datasets import WikiText2\n",
    "from torch import nn, optim\n",
    "from torchtext.legacy import data\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "torch.manual_seed(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    '''Tokenize a string to words'''\n",
    "    return word_tokenize(text)\n",
    "\n",
    "START = '<START>'\n",
    "STOP = '<STOP>'\n",
    "#Load and split data into three parts\n",
    "TEXT = data.Field(lower=True, tokenize=tokenizer, init_token=START, eos_token=STOP)\n",
    "train, valid, test = WikiText2.splits(TEXT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHwu4VMVCdXq",
    "outputId": "4fb02a02-8f07-4239-b1f0-45234cda2b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28907\n"
     ]
    }
   ],
   "source": [
    "#Build a vocabulary from the train dataset\n",
    "TEXT.build_vocab(train)\n",
    "print('Vocabulary size:', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ua9e-OBMCdXs"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# the length of a text feeding to the RNN layer\n",
    "BPTT_LEN = 32           \n",
    "# train, validation, test data\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits((train, valid, test),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                bptt_len=BPTT_LEN,\n",
    "                                                                repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_Cd8shXCdXy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text tensor torch.Size([32, 64])\n",
      "Size of target tensor torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "#Generate a batch of train data\n",
    "batch = next(iter(train_iter))\n",
    "text, target = batch.text, batch.target\n",
    "print('Size of text tensor',text.size())\n",
    "print('Size of target tensor',target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    def __init__(self, lang_config):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.vocab_size = lang_config['vocab_size']\n",
    "        self.emb_size = lang_config['emb_size']\n",
    "        self.hidden_size = lang_config['hidden_size']\n",
    "        self.num_layer = lang_config['num_layer']\n",
    "        self.bidirectional = lang_config['bidirectional']\n",
    "        \n",
    "        self.embedding = None\n",
    "        self.lstm = None\n",
    "        self.linear = None\n",
    "        \n",
    "        ### TODO: \n",
    "        ###    1. Initialize 'self.embedding' with nn.Embedding function and 2 variables we have initialized for you\n",
    "        ###    2. Initialize 'self.lstm' with nn.LSTM function and 3 variables we have initialized for you\n",
    "        ###    3. Initialize 'self.linear' with nn.Linear function and 2 variables we have initialized for you\n",
    "        ### Reference:\n",
    "        ###        https://pytorch.org/docs/stable/nn.html\n",
    "        \n",
    "        ### YOUR CODE HERE (3 lines)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, self.num_layer, bidirectional=self.bidirectional)\n",
    "        self.linear = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.vocab_size)\n",
    "\n",
    "        ### END OF YOUR CODE\n",
    "        \n",
    "    def forward(self, batch_sents, hidden=None):\n",
    "        '''\n",
    "        params:\n",
    "            batch_sents: torch.LongTensor of shape (sequence_len, batch_size)\n",
    "        return:\n",
    "            normalized_score: torch.FloatTensor of shape (sequence_len, batch_size, vocab_size)\n",
    "        '''\n",
    "        normalized_score = None\n",
    "        hidden = hidden\n",
    "        ### TODO:\n",
    "        ###      1. Feed the batch_sents to self.embedding  \n",
    "        ###      2. Feed the embeddings to self.lstm. Remember to pass \"hidden\" into self.lstm, even if it is None. But we will \n",
    "        ###         use \"hidden\" when implementing greedy search.\n",
    "        ###      3. Apply linear transformation to the output of self.lstm\n",
    "        ###      4. Apply 'F.log_softmax' to the output of linear transformation\n",
    "        ###\n",
    "        ### YOUR CODE HERE (4 lines)\n",
    "        emb = self.embedding(batch_sents)\n",
    "        lstm_out, hidden = self.lstm(emb, hidden)\n",
    "        linear_out = self.linear(lstm_out)\n",
    "        normalized_score = F.log_softmax(linear_out, dim=2)\n",
    "        \n",
    "        ### END OF YOUR CODE\n",
    "        return normalized_score, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs):\n",
    "    for n in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        target_num = 0\n",
    "        model.train()\n",
    "        for batch in train_iter:\n",
    "            \n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "            loss = None\n",
    "            \n",
    "            ### we don't consider \"hidden\" here. So according to the default setting, \"hidden\" will be None\n",
    "            ### YOU CODE HERE (~5 lines)\n",
    "            prediction,_ = model(text)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ### END OF YOUR CODE\n",
    "            ##########################################\n",
    "            train_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "\n",
    "        train_loss /= target_num\n",
    "\n",
    "        # monitor the loss of all the predictions\n",
    "        val_loss = 0\n",
    "        target_num = 0\n",
    "        model.eval()\n",
    "        for batch in valid_iter:\n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "            \n",
    "            prediction,_ = model(text)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "            \n",
    "            val_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "        val_loss /= target_num\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(n+1, train_loss, val_loss))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, vocab_size, criterion, test_iter):\n",
    "    '''\n",
    "    params: \n",
    "        model: LSTM model\n",
    "        test_iter: test data\n",
    "    return:\n",
    "        ppl: perplexity \n",
    "    '''\n",
    "    ppl = None\n",
    "    test_loss = 0\n",
    "    target_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "\n",
    "            prediction,_ = model(text)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "            test_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "\n",
    "        test_loss /= target_num\n",
    "        \n",
    "        ### Compute perplexity according to \"test_loss\"\n",
    "        ### Hint: Consider how the loss is computed.\n",
    "        ### YOUR CODE HERE(1 line)\n",
    "        ppl = np.exp(test_loss)\n",
    "        \n",
    "        ### END OF YOUR CODE\n",
    "        return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size':vocab_size,\n",
    "    'emb_size':128,\n",
    "    'hidden_size':128,\n",
    "    'num_layer':1,\n",
    "    'bidirectional': False\n",
    "}\n",
    "\n",
    "LM = LangModel(config)\n",
    "LM = LM.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean')\n",
    "optimizer = optim.Adam(LM.parameters(), lr=1e-3, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 6.0576, Validation Loss: 5.1698\n",
      "Epoch: 2, Training Loss: 5.3879, Validation Loss: 4.9413\n",
      "Epoch: 3, Training Loss: 5.1198, Validation Loss: 4.8540\n",
      "Epoch: 4, Training Loss: 4.9521, Validation Loss: 4.8109\n",
      "Epoch: 5, Training Loss: 4.8314, Validation Loss: 4.7835\n",
      "Epoch: 6, Training Loss: 4.7346, Validation Loss: 4.7642\n",
      "Epoch: 7, Training Loss: 4.6527, Validation Loss: 4.7528\n",
      "Epoch: 8, Training Loss: 4.5825, Validation Loss: 4.7459\n",
      "Epoch: 9, Training Loss: 4.5212, Validation Loss: 4.7448\n",
      "Epoch: 10, Training Loss: 4.4667, Validation Loss: 4.7471\n"
     ]
    }
   ],
   "source": [
    "train(LM, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.41832661548061"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(LM, vocab_size, criterion, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size':vocab_size,\n",
    "    'emb_size':128,\n",
    "    'hidden_size':128,\n",
    "    'num_layer':1,\n",
    "    'bidirectional': True\n",
    "}\n",
    "\n",
    "biLSTM = LangModel(config)\n",
    "biLSTM = biLSTM.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean')\n",
    "optimizer = optim.Adam(biLSTM.parameters(), lr=1e-3, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 3.1737, Validation Loss: 1.3003\n",
      "Epoch: 2, Training Loss: 0.9979, Validation Loss: 0.6369\n",
      "Epoch: 3, Training Loss: 0.5144, Validation Loss: 0.4420\n",
      "Epoch: 4, Training Loss: 0.3227, Validation Loss: 0.3510\n",
      "Epoch: 5, Training Loss: 0.2297, Validation Loss: 0.3055\n",
      "Epoch: 6, Training Loss: 0.1834, Validation Loss: 0.2808\n",
      "Epoch: 7, Training Loss: 0.1582, Validation Loss: 0.2677\n",
      "Epoch: 8, Training Loss: 0.1423, Validation Loss: 0.2609\n",
      "Epoch: 9, Training Loss: 0.1309, Validation Loss: 0.2564\n",
      "Epoch: 10, Training Loss: 0.1214, Validation Loss: 0.2537\n"
     ]
    }
   ],
   "source": [
    "train(biLSTM, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2749883700175868"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(biLSTM, vocab_size, criterion, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 [code][written] **(8 points)**\n",
    "- We will use the hidden vectors (the working memory) of LSTM as the contextual embeddings. Implement ``contextual_embedding`` function.\n",
    "- Use the ``contextual_embedding`` function to get the contextual embeddings of the word \"play\" in three sequences \"to play\", \"dance play\" and \"sing play\". Then calculate the cosine similarity of \"play\" from each pair of sequences \"to play\", \"dance play\" and \"sing play\". Assume that $\\boldsymbol{w}_1$ and $\\boldsymbol{w}_2$ are embeddings of \"play\" in sequences \"to play\" and \"dance play\" respectively. The cosine similarity can be calculated as \n",
    "\\begin{align}\n",
    "similarity = cos(\\theta) = \\frac{\\boldsymbol{w}^{\\rm T}_1\\boldsymbol{w}_2}{||\\boldsymbol{w}_1||_2||\\boldsymbol{w}_2||_2}\n",
    "\\end{align}\n",
    "Give the explanation of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_embedding(model, sentence):\n",
    "    '''\n",
    "    params: \n",
    "        model: LSTM model\n",
    "        sentence -- list[str]: list of tokens, e.g., ['I', 'am',...]\n",
    "    return:\n",
    "        embeddings -- numpy array of shape (length of sentence, word embedding size)\n",
    "    '''\n",
    "    model.eval()\n",
    "    hidden = None\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    embeddings = np.zeros((0,128))\n",
    "    for idx, i in enumerate(sentence):\n",
    "        ID = TEXT.vocab.stoi[i]\n",
    "        out, hidden = model(torch.LongTensor([[ID]]).to(device), hidden)\n",
    "        embeddings = np.append(embeddings, hidden[0].cpu().detach().numpy()[0],axis=0)  \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard LSTM: (1, 2) - 0.7883657376138035, Bidirectional LSTM: (1, 2) - 0.5561252451627524\n",
      "Standard LSTM: (1, 3) - 0.5877902958178445, Bidirectional LSTM: (1, 3) - 0.4479758609576928\n",
      "Standard LSTM: (1, 4) - 0.5768970044671943, Bidirectional LSTM: (1, 4) - 0.35463701236150924\n",
      "Standard LSTM: (2, 1) - 0.7883657376138035, Bidirectional LSTM: (2, 1) - 0.5561252451627524\n",
      "Standard LSTM: (2, 3) - 0.6284939673226192, Bidirectional LSTM: (2, 3) - 0.6285522043522962\n",
      "Standard LSTM: (2, 4) - 0.5742108603788779, Bidirectional LSTM: (2, 4) - 0.5485594132365814\n",
      "Standard LSTM: (3, 1) - 0.5877902958178445, Bidirectional LSTM: (3, 1) - 0.4479758609576928\n",
      "Standard LSTM: (3, 2) - 0.6284939673226192, Bidirectional LSTM: (3, 2) - 0.6285522043522962\n",
      "Standard LSTM: (3, 4) - 0.8601641905526018, Bidirectional LSTM: (3, 4) - 0.7417703338013901\n",
      "Standard LSTM: (4, 1) - 0.5768970044671943, Bidirectional LSTM: (4, 1) - 0.35463701236150924\n",
      "Standard LSTM: (4, 2) - 0.5742108603788779, Bidirectional LSTM: (4, 2) - 0.5485594132365814\n",
      "Standard LSTM: (4, 3) - 0.8601641905526018, Bidirectional LSTM: (4, 3) - 0.7417703338013901\n"
     ]
    }
   ],
   "source": [
    "sink_seq1 = \"wood does not sink in water\" # sink is in index 3\n",
    "sink_seq2 = \"a small water leak will sink the ship\" # sink is in index 5\n",
    "sink_seq3 = \"there are plates in the kitchen sink\" # sink is in the last index\n",
    "sink_seq4 = \"the kitchen sink was full of dirty dishes\" # sink is in index 2\n",
    "\n",
    "### YOUR CODE HERE\n",
    "def cosine(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Each sentence's embeddings for sink\n",
    "# standard LSTM\n",
    "stan_seq1 = contextual_embedding(LM, sink_seq1.split())[3]\n",
    "stan_seq2 = contextual_embedding(LM, sink_seq2.split())[5]\n",
    "stan_seq3 = contextual_embedding(LM, sink_seq3.split())[-1]\n",
    "stan_seq4 = contextual_embedding(LM, sink_seq4.split())[2]\n",
    "\n",
    "# bidirectional LSTM\n",
    "bidi_seq1 = contextual_embedding(biLSTM, sink_seq1.split())[3]\n",
    "bidi_seq2 = contextual_embedding(biLSTM, sink_seq2.split())[5]\n",
    "bidi_seq3 = contextual_embedding(biLSTM, sink_seq3.split())[-1]\n",
    "bidi_seq4 = contextual_embedding(biLSTM, sink_seq4.split())[2]\n",
    "\n",
    "# Calculate cosine similarities for each sink embedding, restrict to standard and bidirectional LSTM\n",
    "# Basically all permutations of the sink embeddings but don't mix between standard and bidirectional\n",
    "results = {}\n",
    "for i in range(1, 5):\n",
    "    for j in range(1, 5):\n",
    "        if i == j:\n",
    "            continue\n",
    "        results[(i, j)] = cosine(locals()['stan_seq' + str(i)], locals()['stan_seq' + str(j)]), cosine(locals()['bidi_seq' + str(i)], locals()['bidi_seq' + str(j)])\n",
    "\n",
    "for k, v in results.items():\n",
    "    print(f\"Standard LSTM: {k} - {v[0]}, Bidirectional LSTM: {k} - {v[1]}\")\n",
    "### END OF YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***write your explanation:***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As bidirectional LSTMs account for both forward and backward directions when calculating word embeddings in comparison to standard LSTMs, they have a better understanding of the context in which a word appears. As a result of this, the similarities between the cosines are smaller than those of the uni-directional LSTM. The contexts in sentences 1-2 and sentences 3-4 are completely different for the word \"sink\" (initial case being the verb and second case being the noun), which is why in the bi-directional LSTM the cosine similarity is lower. In the uni-directional LSTM, it merely accounts for the forward direction, not providing as much context for the word \"sink\" as the bi-directional LSTM does, hence having a higher cosine similarity across all pairs of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cernoBq6CdX9"
   },
   "source": [
    "### Requirements:\n",
    "- This is an individual project. \n",
    "- Write down names and IDs of students with whom you have discussed (if any). \n",
    "- You should **NOT** copy other's answer, once discovered, the person will get **0** in this mini project.\n",
    "- Complete answers and Python code in the ``mini_project.ipynb`` file. \n",
    "- Follow the honor code strictly.\n",
    "- Submit the file before the due on eDimension system."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mini project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
